# WiDS2025
WiDS Datathon 2025

# Comenzando: Instalaci√≥n y Autenticaci√≥n de Kaggle API

Para bajar los datos y no tenerlos en Github, la form√° m√°s sencilla es interactuar con la API p√∫blica de Kaggle a trav√©s de su l√≠nea de comandos (CLI) implementada en Python.

## Instalaci√≥n

Aseg√∫rate de tener Python y el administrador de paquetes `pip` instalados. Luego, ejecuta el siguiente comando para acceder a la API de Kaggle desde la l√≠nea de comandos:

```bash
pip install kaggle
```


Para una instalaci√≥n local en Linux, la ubicaci√≥n predeterminada es `~/.local/bin`. En Windows, la ubicaci√≥n predeterminada es `$PYTHON_HOME/Scripts`.

## Autenticaci√≥n

Para utilizar la API p√∫blica de Kaggle, primero debes autenticarte mediante un token de API.

1. Ve a la pesta√±a "Settings" en tu perfil de usuario de Kaggle.
2. Selecciona "Create New Token".
3. Esto descargar√° un archivo llamado `kaggle.json`, que contiene tus credenciales de la API.

Si est√°s usando la herramienta CLI de Kaggle, esta buscar√° el token en:

* Linux, macOS y otros sistemas basados en UNIX: `~/.kaggle/kaggle.json`
* Windows: `C:\Users\<tu-usuario>\.kaggle\kaggle.json`

Si el token no se encuentra en la ubicaci√≥n correcta, se generar√° un error. Por lo tanto, una vez descargado el token, mu√©velo desde la carpeta de descargas a la ubicaci√≥n correspondiente. En este caso, para descargar los datos de la competencia, col√≥cate en la carpeta `/data` y escribe el siguiente comando,

```bash
kaggle competitions download -c widsdatathon2025
```

**Nota**: Hay que estar registrado en la competencia.

Si utilizas la API de Kaggle directamente, la ubicaci√≥n del token no es relevante, siempre que puedas proporcionar tus credenciales en tiempo de ejecuci√≥n.



## üíª Tareas para mejorar el Pipeline de Aprendizaje Autom√°tico

Cada estudiante debe elegir (o se le asignar√°) una tarea para contribuir a la mejora del c√≥digo existente. Por favor, realiza tus cambios en el m√≥dulo correspondiente o crea uno nuevo si es necesario. Las contribuciones se realizar√°n mediante un pull request de Github.


### ‚úÖ Tareas asignadas

1. **Modularizar los Transformadores de Preprocesamiento**  
   Reestructurar el paso de preprocesamiento (por ejemplo, `StandardScaler`, `MinMaxScaler`) del archivo `module_data.py` en una funci√≥n reutilizable o un pipeline de preprocesamiento.

2. **Agregar Visualizaci√≥n del An√°lisis de Componentes Principales (PCA)**  
   Despu√©s de aplicar PCA, generar gr√°ficos que muestren:
   - Varianza explicada por componente  
   - Varianza explicada acumulada  
   Guardar las figuras en el directorio `plots/`.

3. **Implementar B√∫squeda de Hiperpar√°metros con GridSearch**  
   Agregar funcionalidad en `module_model.py` para realizar ajuste de hiperpar√°metros con `GridSearchCV` para los modelos existentes (por ejemplo, LogisticRegression, RandomForest).

4. **Reestructurar el Bucle de Comparaci√≥n de Modelos**  
   En `run_classification.py`, reemplazar la evaluaci√≥n est√°tica de modelos por un bucle que entrene y eval√∫e m√∫ltiples modelos. Registrar todos los resultados usando MLflow.

5. **Agregar Validaci√≥n Cruzada con Stratified K-Fold**  
   Extender `ModelEvaluation` para soportar `StratifiedKFold`. Calcular m√©tricas promedio (F1, accuracy, etc.) entre los diferentes folds.

6. **Crear un M√≥dulo `model_metrics.py`**  
   Mover los c√°lculos de m√©tricas (F1 score, precisi√≥n, recall, AUC) a un m√≥dulo reutilizable. Este m√≥dulo debe ser utilizado tanto por `ModelEvaluation` como por las evaluaciones de `GridSearchCV`.

7. **Implementar Gr√°ficas de Matriz de Confusi√≥n**  
   Agregar funcionalidad para generar y guardar gr√°ficas de matriz de confusi√≥n para cada modelo evaluado. Guardar los gr√°ficos en el directorio `plots/`.

8. **Crear un Generador de Importancia de Caracter√≠sticas**  
   Extraer y graficar las importancias de caracter√≠sticas para modelos tipo √°rbol (por ejemplo, RandomForest). Guardar las gr√°ficas y archivos CSV ordenados por importancia en `plots/` y `data/`.

9. **Exportar M√©tricas de Evaluaci√≥n a CSV**  
   Despu√©s de cada evaluaci√≥n de modelo, exportar las m√©tricas detalladas (F1, accuracy, precisi√≥n, recall, AUC) a un archivo CSV para su registro y comparaci√≥n.

10. **Hacer Configurable la Estrategia de Imputaci√≥n**  
   Permitir configurar la estrategia de imputaci√≥n (`mean`, `median`, `mode`) mediante par√°metros o archivo de configuraci√≥n, en lugar de dejarlo codificado como `median`.

11. **Automatizar el Manejo del Directorio de submission**  
   Mejorar el m√©todo `ModelSubmission.to_submission()` para que cree archivos o carpetas con fecha y hora, evitando sobrescribir resultados anteriores.

12. **Agregar un M√≥dulo de Selecci√≥n de Caracter√≠sticas (`feature_selection.py`)**  
   Implementar m√©todos de selecci√≥n autom√°tica de variables usando:
   - `SelectKBest` con puntuaciones de chi-cuadrado o ANOVA
   - `Recursive Feature Elimination (RFE)`
   - Importancia de caracter√≠sticas basada en modelos (por ejemplo, RandomForest)
   Permitir elegir el m√©todo y el n√∫mero de caracter√≠sticas por par√°metro.

13. **Evaluaci√≥n el Impacto del Preprocesamiento en el Desempe√±o del Modelo**  
   Comparar distintos pipelines de preprocesamiento (por ejemplo, con o sin escalado, distintas imputaciones o codificaciones) y registrar c√≥mo cambia el desempe√±o del modelo (F1, accuracy, etc.). Documentar resultados en CSV y MLflow.

